import { c as createElementBlock, b as createStaticVNode, o as openBlock } from "./app-BO1sXcn2.js";
import { _ as _export_sfc } from "./plugin-vue_export-helper-1tPrXgE0.js";
const _imports_0 = "/images/huggingface/section1/transformers.svg";
const _sfc_main = {};
function _sfc_render(_ctx, _cache) {
  return openBlock(), createElementBlock("div", null, _cache[0] || (_cache[0] = [
    createStaticVNode('<h2 id="_1-nlp-介绍" tabindex="-1"><a class="header-anchor" href="#_1-nlp-介绍"><span>1. NLP 介绍</span></a></h2><p>NLP 的任务不仅仅是理解单个字词的含义，而是要理解上下文的含义。</p><p>NLP 任务有很多，比如：</p><ul><li><strong>对整个句子进行分类</strong>：获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关</li><li><strong>对句子中的每个词语进行分类</strong>：识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）</li><li><strong>生成上下文</strong>：用自动生成的文本完成提示，用屏蔽词填充文本中的空白</li><li><strong>从文本中提取答案</strong>：给定问题和上下文，根据上下文中提供的信息提取问题的答案</li><li><strong>根据输入文本生成新的句子</strong>：将文本翻译成另一种语言，总结文本</li></ul><h3 id="_1-1-术语-architectures-vs-checkpoints" tabindex="-1"><a class="header-anchor" href="#_1-1-术语-architectures-vs-checkpoints"><span>1.1 术语：Architectures vs. checkpoints</span></a></h3><p>在接下来的学习中，你将会看到 architectures、checkpoints，还有 models 这些术语。</p><ul><li>Architecture: 模型框架。每一层的定义、模型中发生的每个操作的定义。</li><li>Checkpoints: 对于一个给定 architecture 的权重。</li><li>Model: 范语，可能是指 architecture，也可能是指 checkpoints。</li></ul><p>如：BERT 是一个 architecture。bert-base-cased 是由 Google 团队为 BERT 训练的初始权重，它是 checkpoints。我们可以说 BERT model，也可以说 bert-base-cased model.</p><h2 id="_2-transformers-能做什么" tabindex="-1"><a class="header-anchor" href="#_2-transformers-能做什么"><span>2. Transformers 能做什么</span></a></h2><p>你可以使用<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">🤗 Transformers 库</a>来创建并使用公开的模型。你可以在<a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">模型中心</a>中查找预训练模型。你也可以在 Hub 中上传你自己的模型。</p><h3 id="_2-1-快速体验-🤗-transformers-库" tabindex="-1"><a class="header-anchor" href="#_2-1-快速体验-🤗-transformers-库"><span>2.1 快速体验 🤗 Transformers 库</span></a></h3><p>🤗 Transformers 库提供了 <code>pipeline()</code> 函数，它聚合了预训练模型和对应的文本预处理。使用该函数可以直接根据输入返回目标输出。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline</span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 选择任务 sentiment-analysis，创建分类器对象</span></span>\n<span class="line"><span class="token comment"># 没有指定 model，则会使用默认 model</span></span>\n<span class="line">classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;sentiment-analysis&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 1 传入一个句子</span></span>\n<span class="line">classifier<span class="token punctuation">(</span><span class="token string">&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"><span class="token comment"># 结果：[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598048329353333}]</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 2 传入多个句子</span></span>\n<span class="line">classifier<span class="token punctuation">(</span></span>\n<span class="line">    <span class="token punctuation">[</span><span class="token string">&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;I hate this so much!&quot;</span><span class="token punctuation">]</span></span>\n<span class="line"><span class="token punctuation">)</span></span>\n<span class="line"><span class="token comment"># 结果</span></span>\n<span class="line"><span class="token comment"># [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598048329353333},</span></span>\n<span class="line"><span class="token comment">#  {&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9994558691978455}]</span></span>\n<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>目前支持的 pipeline 见 <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">Model 中心</a>。 如果不想使用默认模型，可通过 <code>model</code> 参数传递对应的模型名称。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline</span>\n<span class="line"></span>\n<span class="line">generator <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;text-generation&quot;</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&quot;distilgpt2&quot;</span><span class="token punctuation">)</span></span>\n<span class="line">generator<span class="token punctuation">(</span></span>\n<span class="line">    <span class="token string">&quot;In this course, we will teach you how to&quot;</span><span class="token punctuation">,</span></span>\n<span class="line">    max_length<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span></span>\n<span class="line">    num_return_sequences<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span></span>\n<span class="line"><span class="token punctuation">)</span></span>\n<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-2-局限性-偏见" tabindex="-1"><a class="header-anchor" href="#_2-2-局限性-偏见"><span>2.2 局限性 &amp; 偏见</span></a></h3><p>为了在大规模数据上进行预训练，研究员们会收集尽可能多的数据，这其中可能会夹杂一些意识形态或者价值观的刻板印象。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline</span>\n<span class="line"></span>\n<span class="line">unmasker <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;fill-mask&quot;</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&quot;bert-base-uncased&quot;</span><span class="token punctuation">)</span></span>\n<span class="line">result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">&quot;This man works as a [MASK].&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">&quot;token_str&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line">result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">&quot;This woman works as a [MASK].&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">&quot;token_str&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span></span>\n<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">[&#39;lawyer&#39;, &#39;carpenter&#39;, &#39;doctor&#39;, &#39;waiter&#39;, &#39;mechanic&#39;]</span>\n<span class="line">[&#39;nurse&#39;, &#39;waitress&#39;, &#39;teacher&#39;, &#39;maid&#39;, &#39;prostitute&#39;]</span>\n<span class="line"></span></code></pre></div><p>观察结果，有明显的性别相关性，妓女成为了“女性工作”相关的前五名答案之一。</p><div class="hint-container warning"><p class="hint-container-title">注意</p><p>原始模型中很容易掺杂性别歧视、种族歧视等问题，在模型上进一步微调并不会消除这种偏差。</p></div><h2 id="_3-transformer-背景知识" tabindex="-1"><a class="header-anchor" href="#_3-transformer-背景知识"><span>3. Transformer 背景知识</span></a></h2><p>Transformer 架构于 2017 年 6 月推出。大体上可以将 Transformer 模型分为三类：</p><ul><li>GPT-like (自回归（auto-regressive）Transformer 模型)</li><li>BERT-like (自编码（auto-encoding）Transformer 模型)</li><li>BART/T5-like (序列到序列（sequence-to-sequence）Transformer 模型)</li></ul><h3 id="_3-1-transformer-是语言模型-language-model" tabindex="-1"><a class="header-anchor" href="#_3-1-transformer-是语言模型-language-model"><span>3.1 Transformer 是语言模型（language model）</span></a></h3><p>包括 GPT、BERT、BART、T5 等 Transformer 模型都是语言模型，即他们已经以自监督学习（self-supervised）的方式在大量文本上进行了训练。</p><p>这类模型在其进行训练的语料上进行了理解，但是对于具体问题，它就没那么有针对性了，于是我们需要进行迁移学习（transfer learning）。在迁移学习时，对于具体问题，我们使用人工标注的数据以有监督的方式进行精调（fine-tune）。</p><h3 id="_3-2-transformer-是大模型" tabindex="-1"><a class="header-anchor" href="#_3-2-transformer-是大模型"><span>3.2 Transformer 是大模型</span></a></h3><p>实现更好性能的一般策略是增加模型的大小以及预训练的数据量。</p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" width="60%"><h3 id="_3-3-迁移学习-transfer-learning" tabindex="-1"><a class="header-anchor" href="#_3-3-迁移学习-transfer-learning"><span>3.3 迁移学习（Transfer Learning）</span></a></h3><p>预训练（Pretraining）指从头开始训练模型。这往往需要使用大规模语料，花费长达数周的时间。</p><p>微调（Fine-tuning）是在预训练好的模型上进行进一步的训练。要进行微调，你需要使用预训练模型以及针对特定任务的数据集再次进行训练。进行微调可以有效降低时间、设备成本，使用更小的数据集完成。</p><h2 id="_4-transformer-结构" tabindex="-1"><a class="header-anchor" href="#_4-transformer-结构"><span>4. Transformer 结构</span></a></h2><div class="hint-container info"><p class="hint-container-title">扩展阅读</p><p>推荐 <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a> 这篇文章。在该文章中，作者使用动图清晰地描述了 Transformer 的结构和原理。</p></div><p>Transformer 主要由两部分组成：</p><ul><li>Encoders (编码器): 编码器接收输入并构建其表示（即特征）。这意味着对模型进行了优化，以从输入中获得理解。</li><li>Decoders (解码器): 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。</li></ul><p>这两部分可以单独使用，这取决于你要做什么任务：</p><ul><li><p><strong>Encoder-only 模型（auto-encoding models）</strong>：适用于需要理解输入的任务，如句子分类和命名实体识别。</p><p>这类模型有 <a href="https://huggingface.co/docs/transformers/model_doc/albert" target="_blank" rel="noopener noreferrer">ALBERT</a>, <a href="https://huggingface.co/docs/transformers/model_doc/bert" target="_blank" rel="noopener noreferrer">BERT</a>, <a href="https://huggingface.co/docs/transformers/model_doc/distilbert" target="_blank" rel="noopener noreferrer">DistillBERT</a>, <a href="https://huggingface.co/docs/transformers/model_doc/electra" target="_blank" rel="noopener noreferrer">ELECTRA</a>, <a href="https://huggingface.co/docs/transformers/model_doc/roberta" target="_blank" rel="noopener noreferrer">RoBERTa</a></p></li><li><p><strong>Decoder-only 模型（auto-regressive models）</strong>：适用于生成任务，如文本生成。</p><p>这类模型有 <a href="https://huggingface.co/docs/transformers/model_doc/ctrl" target="_blank" rel="noopener noreferrer">CTRL</a>, <a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt" target="_blank" rel="noopener noreferrer">GPT</a>, <a href="https://huggingface.co/docs/transformers/model_doc/gpt2" target="_blank" rel="noopener noreferrer">GPT-2</a>, <a href="https://huggingface.co/docs/transformers/model_doc/transfo-xl" target="_blank" rel="noopener noreferrer">Transformer XL</a></p></li><li><p><strong>Encoder-decoder 模型（sequence-to-sequence models）</strong>：适用于需要根据输入进行生成的任务，如翻译或摘要。预训练这类模型可以使用 encode 或 decoder 的目标。</p><p>这类模型有 <a href="https://huggingface.co/docs/transformers/model_doc/bart" target="_blank" rel="noopener noreferrer">BART</a>, <a href="https://huggingface.co/docs/transformers/model_doc/mbart" target="_blank" rel="noopener noreferrer">mBART</a>, <a href="https://huggingface.co/docs/transformers/model_doc/marian" target="_blank" rel="noopener noreferrer">Marian</a>, <a href="https://huggingface.co/docs/transformers/model_doc/t5" target="_blank" rel="noopener noreferrer">T5</a></p></li></ul><h3 id="_4-1-注意力层-attention-layers" tabindex="-1"><a class="header-anchor" href="#_4-1-注意力层-attention-layers"><span>4.1 注意力层（Attention Layers）</span></a></h3><p>注意力层使得模型对不同位置的字词有着不同的关注程度。</p><p>比如，在做文本翻译任务时，将 &quot;I like eating apples&quot; 翻译成中文，在翻译 like 时，模型需要关注 I 和 eating 来获得正确的翻译，而对 apples 的关注度可能小一些；翻译 &quot;It feels like a soft blanket&quot; 时，关注 feels 会帮助模型获得正确的翻译。</p><h3 id="_4-2-原始模型" tabindex="-1"><a class="header-anchor" href="#_4-2-原始模型"><span>4.2 原始模型</span></a></h3><p>Transformer 最开始是为了翻译任务而设计的。</p><p>在训练过程中，encoder 和 decoder 分别接收两种语言的同一个句子。encoder 使用注意力层，可以“看到”该句子中的全部字词。而 decoder 只能看到已经翻译好的字词（即在正在被翻译的字词之前已经生成的部分）。 比如 decoder 已经生成了3个单词，在生成第4个单词时，我们会把前三个单词也作为输入，连同 encoder 输出的部分一起作为 decoder 的输入来生成第4个单词。</p><p>为了加快训练，我们会喂给 decoder 完整的目标，但是不允许它使用没有预测的词汇。例如，我们正在预测第4个单词，但是模型看到了目标中的第4个单词，显然这样的模型在实际中不会获得好的效果。</p><p>最初的 Transformer 结构如下：</p><figure><img src="' + _imports_0 + '" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>注意，在 decoder 中，第一个注意力层关注所有 decoder 的过去的输入，第二个注意力层，使用了来自 encoder 的输出。因此它能够获得完整的输入句子来对当前词语进行最佳预测。</p><p>我们还可以使用注意力遮罩层（attention mask）以使得模型关注某些表示。比如，在批处理句子时，会使用填充的方式使句子长度保持一致，填充的内容无意义，我们不希望模型关注它。</p><h2 id="_5-小结" tabindex="-1"><a class="header-anchor" href="#_5-小结"><span>5. 小结</span></a></h2><p>本节内容介绍了 NLP 任务以及如何使用 🤗 Transformers 中的 <code>pipeline()</code> 函数来执行不同的 NLP 任务。你可以在<a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">模型中心</a>中查找模型，按照 Model Card 中的说明或者使用页面上的 inference API 进行使用。</p><p>我们简单介绍了 Transformer 的结构，如果你想做进一步了解，推荐阅读 <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a>。</p>', 53)
  ]));
}
const Chapter1_html = /* @__PURE__ */ _export_sfc(_sfc_main, [["render", _sfc_render], ["__file", "Chapter1.html.vue"]]);
const data = JSON.parse('{"path":"/ai/huggingface-nlp/section1/Chapter1.html","title":"1. Transformer Models","lang":"zh-CN","frontmatter":{"lang":"zh-CN","title":"1. Transformer Models","description":null,"article":false},"headers":[{"level":2,"title":"1. NLP 介绍","slug":"_1-nlp-介绍","link":"#_1-nlp-介绍","children":[{"level":3,"title":"1.1 术语：Architectures vs. checkpoints","slug":"_1-1-术语-architectures-vs-checkpoints","link":"#_1-1-术语-architectures-vs-checkpoints","children":[]}]},{"level":2,"title":"2. Transformers 能做什么","slug":"_2-transformers-能做什么","link":"#_2-transformers-能做什么","children":[{"level":3,"title":"2.1 快速体验 🤗 Transformers 库","slug":"_2-1-快速体验-🤗-transformers-库","link":"#_2-1-快速体验-🤗-transformers-库","children":[]},{"level":3,"title":"2.2 局限性 & 偏见","slug":"_2-2-局限性-偏见","link":"#_2-2-局限性-偏见","children":[]}]},{"level":2,"title":"3. Transformer 背景知识","slug":"_3-transformer-背景知识","link":"#_3-transformer-背景知识","children":[{"level":3,"title":"3.1 Transformer 是语言模型（language model）","slug":"_3-1-transformer-是语言模型-language-model","link":"#_3-1-transformer-是语言模型-language-model","children":[]},{"level":3,"title":"3.2 Transformer 是大模型","slug":"_3-2-transformer-是大模型","link":"#_3-2-transformer-是大模型","children":[]},{"level":3,"title":"3.3 迁移学习（Transfer Learning）","slug":"_3-3-迁移学习-transfer-learning","link":"#_3-3-迁移学习-transfer-learning","children":[]}]},{"level":2,"title":"4. Transformer 结构","slug":"_4-transformer-结构","link":"#_4-transformer-结构","children":[{"level":3,"title":"4.1 注意力层（Attention Layers）","slug":"_4-1-注意力层-attention-layers","link":"#_4-1-注意力层-attention-layers","children":[]},{"level":3,"title":"4.2 原始模型","slug":"_4-2-原始模型","link":"#_4-2-原始模型","children":[]}]},{"level":2,"title":"5. 小结","slug":"_5-小结","link":"#_5-小结","children":[]}],"git":{"createdTime":1735022911000,"updatedTime":1735022911000,"contributors":[{"name":"Hertz","username":"Hertz","email":"hanzhuosoul@gmail.com","commits":1,"url":"https://github.com/Hertz"}]},"readingTime":{"minutes":6.94,"words":2083},"filePathRelative":"ai/huggingface-nlp/section1/Chapter1.md","localizedDate":"2024年12月24日","excerpt":"<h2>1. NLP 介绍</h2>\\n<p>NLP 的任务不仅仅是理解单个字词的含义，而是要理解上下文的含义。</p>\\n<p>NLP 任务有很多，比如：</p>\\n<ul>\\n<li><strong>对整个句子进行分类</strong>：获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关</li>\\n<li><strong>对句子中的每个词语进行分类</strong>：识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）</li>\\n<li><strong>生成上下文</strong>：用自动生成的文本完成提示，用屏蔽词填充文本中的空白</li>\\n<li><strong>从文本中提取答案</strong>：给定问题和上下文，根据上下文中提供的信息提取问题的答案</li>\\n<li><strong>根据输入文本生成新的句子</strong>：将文本翻译成另一种语言，总结文本</li>\\n</ul>"}');
export {
  Chapter1_html as comp,
  data
};
